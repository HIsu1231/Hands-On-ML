{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handsonml_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpe0mxLTwfydAbFG+X7uFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HIsu1231/Hands-On-ML/blob/main/handsonml_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UCHaGyiuTQ1",
        "outputId": "2cace4b8-e828-4aba-b37e-9086e6d783e0"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "  %pip install -q -U tfx\n",
        "  print(\"패키지 호환 에러는 무시해도 괜찮습니다.\")\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.4 MB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 62.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 203 kB 54.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.0 MB 85 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 36.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.7 MB 79 kB/s \n",
            "\u001b[K     |████████████████████████████████| 135 kB 75.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 13.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 49.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 44.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 783 kB/s \n",
            "\u001b[K     |████████████████████████████████| 249 kB 65.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 40.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 183 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 69.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 267 kB 72.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 76.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 169 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 435 kB 64.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 255 kB 73.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 105 kB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 202 kB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 202 kB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 201 kB 73.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 201 kB 70.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 201 kB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 92 kB 360 kB/s \n",
            "\u001b[K     |████████████████████████████████| 92 kB 368 kB/s \n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 294 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 790 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 374 kB 56.3 MB/s \n",
            "\u001b[?25h  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.29.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.21 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "패키지 호환 에러는 무시해도 괜찮습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spjCc2M0xjqU"
      },
      "source": [
        "##데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6B0Tc-RuuMf",
        "outputId": "4cd70250-485e-40b0-f09e-8f48825f7a81"
      },
      "source": [
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "#from_tensor_slices(): 텐서를 받아 X의 각 원소가 아이템으로 표현되는 tf.data.Dataset을 만듦\n",
        "#dataset = tf.data.Dataset.range(10)과 동일\n",
        "dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rJ_1-3exppF"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKULxXdTxrq1",
        "outputId": "d64ec48f-ddc2-47a7-b156-c905e9f89f31"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(3, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n",
            "tf.Tensor(5, shape=(), dtype=int64)\n",
            "tf.Tensor(6, shape=(), dtype=int64)\n",
            "tf.Tensor(7, shape=(), dtype=int64)\n",
            "tf.Tensor(8, shape=(), dtype=int64)\n",
            "tf.Tensor(9, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dap73fvxudf",
        "outputId": "a6ec1013-3dd8-4c20-aa25-f53b817720d3"
      },
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "\n",
        "#repeat(): 원본 데이터셋의 아이템을 파라미터값만큼 반복하는 새로운 데이터셋 반환\n",
        "#batch(): 데이터셋의 아이템을 파라미터 값만큼의 그룹으로 묶음\n",
        "#batch() 메서드를 drop_remainder = True로 호출하면 길이가 모자란 마지막 배치 버리고 모든 배치를 동일한 크기로 맞춤\n",
        "\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh96SwTex0QS"
      },
      "source": [
        "dataset = dataset.map(lambda x: x * 2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgLPBRWozUzl",
        "outputId": "653fb6c3-1fd7-4df7-a308-d0bb278dbfe5"
      },
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57B6XKRzYkG"
      },
      "source": [
        "dataset = dataset.unbatch()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Ut0Dhrz-wD"
      },
      "source": [
        "dataset = dataset.filter(lambda x: x < 10)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZY4acZf0Gev",
        "outputId": "f6dc4394-4bb3-4785-cc20-c0d1949448b1"
      },
      "source": [
        "for item in dataset.take(3):\n",
        "  print(item)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor(2, shape=(), dtype=int64)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeut875i0NW1",
        "outputId": "a19f034d-d42a-453f-c39c-9ee4f4ed862c"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7)\n",
        "\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\n",
            "tf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n",
            "tf.Tensor([6 9], shape=(2,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H39jxNMP05Ca"
      },
      "source": [
        "###캘리포니아 주택 데이터셋을 여러개의 csv로 나누기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB6_N7tC0XcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a49c671-0828-4b8d-8487-897758fa5213"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jtpuPZt1lu8"
      },
      "source": [
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "  housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "  os.makedirs(housing_dir, exist_ok=True)\n",
        "  path_format = os.path.join(housing_dir, 'my_{}_{:02d}.csv')\n",
        "\n",
        "  filepaths = []\n",
        "  m = len(data)\n",
        "  for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "    part_csv = path_format.format(name_prefix, file_idx)\n",
        "    filepaths.append(part_csv)\n",
        "    with open(part_csv, \"wt\", encoding='utf-8') as f:\n",
        "      if header is not None:\n",
        "        f.write(header)\n",
        "        f.write('\\n')\n",
        "      for row_idx in row_indices:\n",
        "        f.write(','.join([repr(col) for col in data[row_idx]]))\n",
        "        f.write('\\n')\n",
        "  return filepaths"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTO8K7lCD053"
      },
      "source": [
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = '.'.join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, 'train', header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, 'valid', header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, 'test', header, n_parts=10)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "lIyufEr5ENkR",
        "outputId": "ab4528bb-71b3-499e-d0fc-90c4171c87f1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.read_csv(train_filepaths[0]).head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>MedInc.HouseAge.AveRooms.AveBedrms.Population.AveOccup.Latitude.Longitude.MedianHouseValue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3.5214</th>\n",
              "      <th>15.0</th>\n",
              "      <th>3.049945</th>\n",
              "      <th>1.106548</th>\n",
              "      <th>1447.0</th>\n",
              "      <th>1.605993</th>\n",
              "      <th>37.63</th>\n",
              "      <th>-122.43</th>\n",
              "      <td>1.442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.3275</th>\n",
              "      <th>5.0</th>\n",
              "      <th>6.490060</th>\n",
              "      <th>0.991054</th>\n",
              "      <th>3464.0</th>\n",
              "      <th>3.443340</th>\n",
              "      <th>33.69</th>\n",
              "      <th>-117.39</th>\n",
              "      <td>1.687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.1000</th>\n",
              "      <th>29.0</th>\n",
              "      <th>7.542373</th>\n",
              "      <th>1.591525</th>\n",
              "      <th>1328.0</th>\n",
              "      <th>2.250847</th>\n",
              "      <th>38.44</th>\n",
              "      <th>-122.98</th>\n",
              "      <td>1.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.1736</th>\n",
              "      <th>12.0</th>\n",
              "      <th>6.289003</th>\n",
              "      <th>0.997442</th>\n",
              "      <th>1054.0</th>\n",
              "      <th>2.695652</th>\n",
              "      <th>33.55</th>\n",
              "      <th>-117.70</th>\n",
              "      <td>2.621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0549</th>\n",
              "      <th>13.0</th>\n",
              "      <th>5.312457</th>\n",
              "      <th>1.085092</th>\n",
              "      <th>3297.0</th>\n",
              "      <th>2.244384</th>\n",
              "      <th>33.93</th>\n",
              "      <th>-116.93</th>\n",
              "      <td>0.956</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             MedInc.HouseAge.AveRooms.AveBedrms.Population.AveOccup.Latitude.Longitude.MedianHouseValue\n",
              "3.5214 15.0 3.049945 1.106548 1447.0 1.605993 37.63 -122.43                                              1.442                                         \n",
              "5.3275 5.0  6.490060 0.991054 3464.0 3.443340 33.69 -117.39                                              1.687                                         \n",
              "3.1000 29.0 7.542373 1.591525 1328.0 2.250847 38.44 -122.98                                              1.621                                         \n",
              "7.1736 12.0 6.289003 0.997442 1054.0 2.695652 33.55 -117.70                                              2.621                                         \n",
              "2.0549 13.0 5.312457 1.085092 3297.0 2.244384 33.93 -116.93                                              0.956                                         "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mat28N8PEVcA",
        "outputId": "a44cbe9e-7d15-40d0-caf9-8731a523e96f"
      },
      "source": [
        "with open(train_filepaths[0]) as f:\n",
        "  for i in range(5):\n",
        "        print(f.readline(), end='')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedInc.HouseAge.AveRooms.AveBedrms.Population.AveOccup.Latitude.Longitude.MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dATa0pE_Eb0i",
        "outputId": "b21502f0-7b61-4892-fb64-d191a2052631"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhniW18tE2bF"
      },
      "source": [
        "###입력 파이프라인 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr1v5UnGEeVb"
      },
      "source": [
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
        "\n",
        "#list_files(): 파일 경로를 섞은 데이터셋 반환"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWZVUSNCE8Xd",
        "outputId": "cad5ac2a-a8ff-4d98-e8f1-b714f62e3d2e"
      },
      "source": [
        "for filepath in filepath_dataset:\n",
        "  print(filepath)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'datasets/housing/my_train_15.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_08.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_03.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_01.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_10.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_05.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_19.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_16.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_02.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_09.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_00.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_07.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_12.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_04.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_17.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_11.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_14.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_18.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_06.csv', shape=(), dtype=string)\n",
            "tf.Tensor(b'datasets/housing/my_train_13.csv', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tontLCQiE-ut"
      },
      "source": [
        "n_readers = 5\n",
        "#interleav(): filepath_dataset에 있는 파일 경로에서 데이터를 읽는 데이터셋 생성\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtNaMqdDFKoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e139b54-a5ac-4d37-edcb-5fffe5af3039"
      },
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\n",
            "b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\n",
            "b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\n",
            "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
            "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4_aHyYVFM_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c923f1d-3e62-424d-81da-4fd420fadbdd"
      },
      "source": [
        "record_defaults = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'Hello', tf.constant([])]\n",
        "parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)\n",
        "parsed_fields"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
              " <tf.Tensor: shape=(), dtype=float64, numpy=3.0>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'4'>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kndrKgRpp6Ai",
        "outputId": "70dc893c-9ece-4401-88e0-42c3fc6863b7"
      },
      "source": [
        "parsed_fields = tf.io.decode_csv(',,,,5', record_defaults)\n",
        "parsed_fields"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
              " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
              " <tf.Tensor: shape=(), dtype=string, numpy=b'Hello'>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zyuoQrCqeyd",
        "outputId": "45d036b2-c0d4-4512-f2f8-c042a8bd27bf"
      },
      "source": [
        "try:\n",
        "  parsed_fields = tf.io.decode_csv(',,,,', record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "  print(ex)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmxDaqH-qsTi",
        "outputId": "9c4fcb3e-471c-46df-867d-d8a38e2d322a"
      },
      "source": [
        "try:\n",
        "  parsed_fields = tf.io.decode_csv('1,2,3,4,5,6,7', record_defaults)\n",
        "except tf.errors.InvalidArgumentError as ex:\n",
        "  print(ex)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expect 5 fields but have 7 in record 0 [Op:DecodeCSV]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pvbBxZVq0ku"
      },
      "source": [
        "n_inputs = 8 #X_train.shape[-1]\n",
        "\n",
        "@tf.function\n",
        "def preprocess(line):\n",
        "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "  x = tf.stack(fields[:-1])\n",
        "  y = tf.stack(fields[-1:])\n",
        "  return (x - X_mean) / X_std, y"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9L45U41seRJ",
        "outputId": "3e44b599-4f87-4f68-e6c0-b9bf9b8bae8a"
      },
      "source": [
        "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
              "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pYHFHqswL6"
      },
      "source": [
        "###데이터 적재와 전처리를 합치기\n",
        "\n",
        "csv파일에서 캘리포니아 주택 데이터셋을 효율적으로 적재하고 전처리, 셔플링, 반복, 배치를 적용한 데이터셋을 만들어 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDfrE2ajsmda"
      },
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "  dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
        "  dataset = dataset.interleave(\n",
        "      lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "      cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "  \n",
        "  dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "  dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset.prefetch(1)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDwSWxFPthvd",
        "outputId": "ebc9c2eb-26d4-4e44-d694-73f73df0c796"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_set = csv_reader_dataset(train_filepaths, batch_size=3)\n",
        "for X_batch, y_batch in train_set.take(2):\n",
        "  print(\"X = \", X_batch)\n",
        "  print(\"y = \", y_batch)\n",
        "  print()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X =  tf.Tensor(\n",
            "[[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  0.00604472\n",
            "   1.2525111  -1.3671792 ]\n",
            " [ 5.818099    1.8491895   1.1784915   0.28173092 -1.2496178  -0.3571987\n",
            "   0.7231292  -1.0023477 ]\n",
            " [-0.9253566   0.5834586  -0.7807257  -0.28213993 -0.36530012  0.27389365\n",
            "  -0.76194876  0.72684526]], shape=(3, 8), dtype=float32)\n",
            "y =  tf.Tensor(\n",
            "[[1.752]\n",
            " [1.313]\n",
            " [1.535]], shape=(3, 1), dtype=float32)\n",
            "\n",
            "X =  tf.Tensor(\n",
            "[[-0.8324941   0.6625668  -0.20741376 -0.18699841 -0.14536144  0.09635526\n",
            "   0.9807942  -0.67250353]\n",
            " [-0.62183803  0.5834586  -0.19862501 -0.3500319  -1.1437552  -0.3363751\n",
            "   1.107282   -0.8674123 ]\n",
            " [ 0.8683102   0.02970133  0.3427381  -0.29872298  0.7124906   0.28026953\n",
            "  -0.72915536  0.86178064]], shape=(3, 8), dtype=float32)\n",
            "y =  tf.Tensor(\n",
            "[[0.919]\n",
            " [1.028]\n",
            " [2.182]], shape=(3, 1), dtype=float32)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUw2FruBuoIM"
      },
      "source": [
        "###tf.keras와 데이터셋 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkKABQecuD-s"
      },
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FBs32eyu1vZ"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1),                         \n",
        "])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uzBI4azvHwU"
      },
      "source": [
        "model.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SWSsViNvVxq",
        "outputId": "4ccc0d0c-f96e-4688-8f24-772a2c6a8d4a"
      },
      "source": [
        "batch_size = 32\n",
        "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10, validation_data=valid_set)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "362/362 [==============================] - 2s 3ms/step - loss: 1.4679 - val_loss: 21.5124\n",
            "Epoch 2/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.8735 - val_loss: 0.6648\n",
            "Epoch 3/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.6317 - val_loss: 0.6196\n",
            "Epoch 4/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5933 - val_loss: 0.5669\n",
            "Epoch 5/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5629 - val_loss: 0.5402\n",
            "Epoch 6/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5693 - val_loss: 0.5209\n",
            "Epoch 7/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5231 - val_loss: 0.6130\n",
            "Epoch 8/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.5074 - val_loss: 0.4818\n",
            "Epoch 9/10\n",
            "362/362 [==============================] - 1s 3ms/step - loss: 0.4963 - val_loss: 0.4904\n",
            "Epoch 10/10\n",
            "362/362 [==============================] - 1s 2ms/step - loss: 0.5023 - val_loss: 0.4585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff483a010d0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2S7ARMx1vcNn",
        "outputId": "79d98d7e-0036-4f7d-cdb0-70767c8538ca"
      },
      "source": [
        "model.evaluate(test_set, steps=len(X_test) // batch_size)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - 0s 2ms/step - loss: 0.4788\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4787752628326416"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5PhnpNavlNJ",
        "outputId": "0b5da569-c380-4389-a8ab-7bf9e62c0998"
      },
      "source": [
        "new_set = test_set.map(lambda X, y: X)\n",
        "X_new = X_test\n",
        "model.predict(new_set, steps=len(X_new) // batch_size)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.3576407],\n",
              "       [2.2552912],\n",
              "       [1.4437604],\n",
              "       ...,\n",
              "       [0.5654392],\n",
              "       [3.9442449],\n",
              "       [1.0232248]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGD36pD2vy8x",
        "outputId": "313e55f6-fe68-44d7-b0bc-234da4c6257f"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "n_epochs = 5\n",
        "batch_size = 32\n",
        "n_steps_per_epoch = len(X_train) // batch_size\n",
        "total_steps = n_epochs * n_steps_per_epoch\n",
        "global_step = 0\n",
        "\n",
        "for X_batch, y_batch in train_set.take(total_steps):\n",
        "  global_step += 1\n",
        "  print(\"\\rGlobal Step {}/{}\".format(global_step, total_steps), end='')\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(X_batch)\n",
        "    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "    loss = tf.add_n([main_loss] + model.losses)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Step 1810/1810"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo7D5Lxbwolu"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHnYMe-iw6Cj"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32,\n",
        "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
        "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
        "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "    for X_batch, y_batch in train_set:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "train(model, 5)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiT0Y-aOx0Lj"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BekYG1s0yR0Z",
        "outputId": "c53058d9-9592-4aac-afac-e50a0433419a"
      },
      "source": [
        "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32,\n",
        "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
        "  train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
        "                                 n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                                 n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "  \n",
        "  n_steps_per_epoch = len(X_train) // batch_size\n",
        "  total_steps = n_epochs * n_steps_per_epoch\n",
        "  global_step = 0\n",
        "\n",
        "  for X_batch, y_batch in train_set.take(total_steps):\n",
        "    global_step += 1\n",
        "    if tf.equal(global_step % 100, 0):\n",
        "      tf.print('\\rGlobal step', global_step, '/', total_steps)\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = model(X_batch)\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "      loss = tf.add_n([main_loss] + model.losses)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "train(model, 5)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global step 100 / 1810\n",
            "Global step 200 / 1810\n",
            "Global step 300 / 1810\n",
            "Global step 400 / 1810\n",
            "Global step 500 / 1810\n",
            "Global step 600 / 1810\n",
            "Global step 700 / 1810\n",
            "Global step 800 / 1810\n",
            "Global step 900 / 1810\n",
            "Global step 1000 / 1810\n",
            "Global step 1100 / 1810\n",
            "Global step 1200 / 1810\n",
            "Global step 1300 / 1810\n",
            "Global step 1400 / 1810\n",
            "Global step 1500 / 1810\n",
            "Global step 1600 / 1810\n",
            "Global step 1700 / 1810\n",
            "Global step 1800 / 1810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsablF6zWx3"
      },
      "source": [
        "Dataset 클래스에 있는 메서드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFyZlQSbzQj5",
        "outputId": "8893b2bc-348b-409f-96e0-87f793ea53d5"
      },
      "source": [
        "for m in dir(tf.data.Dataset):\n",
        "  if not(m.startswith(\"_\") or m.endswith(\"_\")):\n",
        "    func = getattr(tf.data.Dataset, m)\n",
        "    if hasattr(func, \"__doc__\"):\n",
        "       print(\"● {:21s}{}\".format(m + \"()\", func.__doc__.split(\"\\n\")[0]))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "● apply()              Applies a transformation function to this dataset.\n",
            "● as_numpy_iterator()  Returns an iterator which converts all elements of the dataset to numpy.\n",
            "● batch()              Combines consecutive elements of this dataset into batches.\n",
            "● bucket_by_sequence_length()A transformation that buckets elements in a `Dataset` by length.\n",
            "● cache()              Caches the elements in this dataset.\n",
            "● cardinality()        Returns the cardinality of the dataset, if known.\n",
            "● concatenate()        Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
            "● element_spec()       The type specification of an element of this dataset.\n",
            "● enumerate()          Enumerates the elements of this dataset.\n",
            "● filter()             Filters this dataset according to `predicate`.\n",
            "● flat_map()           Maps `map_func` across this dataset and flattens the result.\n",
            "● from_generator()     Creates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\n",
            "● from_tensor_slices() Creates a `Dataset` whose elements are slices of the given tensors.\n",
            "● from_tensors()       Creates a `Dataset` with a single element, comprising the given tensors.\n",
            "● get_single_element() Returns the single element of the `dataset` as a nested structure of tensors.\n",
            "● group_by_window()    Groups windows of elements by key and reduces them.\n",
            "● interleave()         Maps `map_func` across this dataset, and interleaves the results.\n",
            "● list_files()         A dataset of all files matching one or more glob patterns.\n",
            "● map()                Maps `map_func` across the elements of this dataset.\n",
            "● options()            Returns the options for this dataset and its inputs.\n",
            "● padded_batch()       Combines consecutive elements of this dataset into padded batches.\n",
            "● prefetch()           Creates a `Dataset` that prefetches elements from this dataset.\n",
            "● random()             Creates a `Dataset` of pseudorandom values.\n",
            "● range()              Creates a `Dataset` of a step-separated range of values.\n",
            "● reduce()             Reduces the input dataset to a single element.\n",
            "● repeat()             Repeats this dataset so each original value is seen `count` times.\n",
            "● scan()               A transformation that scans a function across an input dataset.\n",
            "● shard()              Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
            "● shuffle()            Randomly shuffles the elements of this dataset.\n",
            "● skip()               Creates a `Dataset` that skips `count` elements from this dataset.\n",
            "● snapshot()           API to persist the output of the input dataset.\n",
            "● take()               Creates a `Dataset` with at most `count` elements from this dataset.\n",
            "● take_while()         A transformation that stops dataset iteration based on a `predicate`.\n",
            "● unbatch()            Splits elements of a dataset into multiple elements.\n",
            "● unique()             A transformation that discards duplicate elements of a `Dataset`.\n",
            "● window()             Returns a dataset of \"windows\".\n",
            "● with_options()       Returns a new `tf.data.Dataset` with the given options set.\n",
            "● zip()                Creates a `Dataset` by zipping together the given datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCxMMX4vzlGS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}