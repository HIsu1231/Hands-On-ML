{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1jrvTTYEmAajToMppXMB6HpT_E9Yr2MWi",
      "authorship_tag": "ABX9TyMGHDR9SaKByVB45s0olEAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HIsu1231/Hands-On-ML/blob/main/Chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dA_NweNX5R9",
        "outputId": "c6d517a4-7e50-40d8-83a0-15595ca8ed5e"
      },
      "source": [
        "!git clone https://github.com/HIsu1231/Hands-On-ML.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hands-On-ML'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIhZg9AyOpe8"
      },
      "source": [
        "###1.1 머신러닝이란?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJApZUmVOtvM"
      },
      "source": [
        "일반적인 정의: 명시적인 프로그래밍 없이 컴퓨터가 학습하는 능력을 갖추게 하는 연구 분야이다.  (아서 새뮤얼)\r\n",
        "\r\n",
        "공학적인 정의: 어떤 작업 T에 대한 컴퓨터 프로그램의 성능 P를 측정했을 때 경험 E로 인해 성능이 향상됐다면, 이 컴퓨터 프로그램은 작업 T와 성능 측정 P에 대해 경험 E로 학습한 것이다. (톰 미첼)\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWzubwN9Sdhb"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "training set: 시스템이 학습하는 데 사용되는 샘플\r\n",
        "\r\n",
        "training instance: 각 훈련 데이터\r\n",
        "\r\n",
        "accuracy: 정확히 분류된 데이터의 비율.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNZ0jxu8QQDI"
      },
      "source": [
        "###1.2 왜 머신러닝을 사용하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLGGLfc9QSqN"
      },
      "source": [
        "전통적인 프로그래밍 기법으로 스팸 필터를 만든다면 규칙이 점점 길고 복잡해지므로 유지보수를 하기가 힘들어짐.\r\n",
        "\r\n",
        "하지만 머신러닝 기법에 기반을 둔 스팸 필터는 스팸 메일을 판단하는 기준을 자동으로 학습함. 따라서 프로그램이 짧아지고 유지보수가 쉬어짐. 정확도가 높아짐.\r\n",
        "\r\n",
        "음성인식 또한 머신러닝이 유용한 또 다른 분야. 각 철자 피치의 사운드 강도를 측정하는 알고리즘보다는 각 단어를 녹음한 샘플을 사용해 스스로 학습하는 알고리즘이 더 좋은 솔루션.\r\n",
        "\r\n",
        "데이터 마이닝: 머신러닝 기술을 적용해서 대용량의 데이터를 분석하여 겉으로는 보이지 않던 패턴을 발견하는 기법. 즉 이 기법을 통해 알고리즘이 학습한 것을 조사 가능함.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Egu3yw9SVtp"
      },
      "source": [
        "뛰어난 분야 요약\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   기존 솔루션으로는 많은 수동 조정과 규칙이 필요한 문제: 머신러닝을 통해 코드를 간단하게 만듦 \r\n",
        "*   전통적인 방식으로는 해결방법이 없는 복잡한 문제\r\n",
        "\r\n",
        "*   유동적인 환경\r\n",
        "*   복잡한 문제와 대용량 데이터에서 통찰 얻기(데이터 마이닝)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKbDRgdSQPS"
      },
      "source": [
        "###1.4 머신러닝 시스템의 종류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEHsQCarSi9b"
      },
      "source": [
        "넓은 범주로 분류하면\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*   사람의 감독하에 훈련하는 것인지 그렇지 않은 것인지 : 지도, 비지도, 준지도, 강화 학습\r\n",
        "*   실시간으로 점진적인 학습을 하는지 아닌지: 온라인 학습, 배치 학습\r\n",
        "*   단순하게 알고 있는 데이터 포인트와 새 데이터 포인트를 비교하는 것인지 or 데훈련 데이터셋에서 패턴을 발견하여 예측 모델을 만드는지: 사례 기반 학습, 모델 기반 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yNfsUrKS8af"
      },
      "source": [
        "1.4.1 지도 학습과 비지도 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f44-chk7THyX"
      },
      "source": [
        "학습하는 동안의 감독 형태나 정보량에 따라 분류.\r\n",
        "\r\n",
        "1) 지도 학습\r\n",
        "\r\n",
        "훈련 데이터에 레이블이 포함됨\r\n",
        "\r\n",
        "    <종류>\r\n",
        "    * 분류 (스팸 필터)\r\n",
        "    * 회귀: feature을 사용하여 target 수치를 예측하는 것\r\n",
        "        - 로지스틱 회귀: 클래스에 속할 확률 출력 (분류에 사용되는 회귀 알고리즘)\r\n",
        "\r\n",
        "2) 비지도 학습\r\n",
        "\r\n",
        "훈련 데이터에 레이블 미포함\r\n",
        "\r\n",
        "    <종류>\r\n",
        "    * 계층 군집: 각 그룹을 더 작은 그룹으로 세분화 시킴\r\n",
        "    * 시각화: 대규모의 고차원 데이터를 2D or 3D로 표현. \r\n",
        "      가능한 구조를 그대로 유지하려 하여 데이터가 어떻게 조직되어있는지 이해 가능. 예상하지 못한 패턴 발견 가능.\r\n",
        "    * 차원 축소: 정보를 잃지 않으면서 데이터를 간소화 -> 상관관계가 있는 여러 특성을 하나로 합침(특성 추출)\r\n",
        "    * 이상치 탐지: 학습 알고리즘에 주입하기 전에 데이터 셋에서 이상한 값을 자동으로 제거\r\n",
        "    * 특이치 탐지: 훈련 세트에 있는 모든 샘플과 달라 보이는 새로운 샘플을 탐지\r\n",
        "    * 연관 규칙 학습: 대량의 데이터에서 특성 간의 흥미로운 관계를 찾음\r\n",
        "        (ex; 바비큐 소스와 감자를 구매한 사람이 스테이크도 구매하는 경향이 있다. 서로 가까이 진열하자)\r\n",
        "\r\n",
        "3) 준지도 학습\r\n",
        "\r\n",
        "일부만 레이블이 있는 데이터를 다룸(지도 학습과 비지도 학습의 조함)\r\n",
        "\r\n",
        "    * 심층 신뢰 신경망(DBN; deep belif network): 여러 겹으로 쌓은 (비지도 학습인)제한된 볼츠만 머신(RBM; restricted Bolzmann machine)에 기초함. \r\n",
        "      RBM이 비지도 학습 방식으로 훈련된 다음 전체 시스템이 지도 학습 방식으로 조정됨\r\n",
        "\r\n",
        "4) 강화 학습\r\n",
        "\r\n",
        "환경을 학습해서 행동을 실행하고 그 결과로 reward(또는 penalty)를 받는 형식의 알고리즘\r\n",
        "\r\n",
        "    \r\n",
        "    에이전트: 학습하는 시스템\r\n",
        "    정책(policy): 주어진 상황에서 에이전트가 어떤 행동을 선택해야 할지 정의\r\n",
        "    -> 시간이 지나면서 가장 큰 보상을 얻기위해 최상의 전략(policy)를 스스로 학습\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfOPgCkbW-3M"
      },
      "source": [
        "1.4.2. 배치 학습과 온라인 학습"
      ]
    }
  ]
}